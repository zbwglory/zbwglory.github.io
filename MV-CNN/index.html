<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Real-time Action Recognition with Enhanced Motion Vector CNNs</title>
	<meta content="Bowen Zhang, zbwglory.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  background: #eee;
}

h1 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18pt;
  font-weight: 700;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  # font-size: 16px;
  font-weight:bold;
}

ul {
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

img.pipeline {
  float: center;
  width: 770px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'zbwglory.github.io');
  ga('send', 'pageview');

</script>
<body>

<div align = "center">
<h1>Real-time Action Recognition with Enhanced Motion Vector CNNs</h1> <br />
<h3>Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao and Hanli Wang</h3>
</div>

<div style="clear: both;">
<div class="paper">
  <img class="pipeline" src="framework.jpg" title="" />
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Abstract</h2>
  <div class="paper">
		The deep two-stream architecture exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method.
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Method</h2>
  <div class="paper">
		In this paper, we purpose to use motion vector to accelerate the processing speed of two-stream ConvNets. However, directly using motion vector as input achieves inferior performance. To improve the performance, enhanced motion vector CNN is purposed. Our insight behind this algorithm is that we find both motion vector and optical flow share some common knowledge, though they belongs to different domain. The knowledge of optical flow CNN can enhance the performance of motion vector CNN.
  <br>
	<br>
  <li style = "list-style-type:square; margin-left:0px">
  <strong>Motion vector CNN: </strong>  We first use ffmpeg to extract motion vectors from videos. Then, we choose the Clarifai network architecture and train the model parameters from scratch.
  </li>
	<br>
  <li style = "list-style-type:square; margin-left:0px">
  <strong>Enhanced motion vector CNN: </strong>  We first use optical flows to train an optical flow CNN (OF-CNN) and employ OF-CNN as pre-trained model. Then, we fine-tune the model parameters of enhanced motion vector CNN (EMV-CNN) using two losses. The first one is called ground truth loss which is supervised by ground truth label. The second one is softmax cross entropy loss which is supervised by the output of optical flow CNN.
  </li>

  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Results</h2>
  <div class="paper">
		<li><h3>Examples of Motion Vectors and Optical Flows</h3> </li>
		<div align=center> <img src="OF_MV.jpg"  width="400" /> </div>
	  <br>
		<li><h3>Speed comparison of optical flow and motion vectors</h3> </li>
<div align=center> <img src="speed.jpg"  width="400" /> </div>
		<br>

		<li><h3>Performance of OF-CNN, MV-CNN and EMV-CNN on UCF-101</h3> </li>
		<div align=center> <img src="OF_MV_EMV.jpg"  width="400" /> </div>
		<br>

  <li><h3>Performance of our method on UCF-101 and THUMOS14</h3> </li>
	<div align=center>
  <img src="UCF-101.jpg"  width="350" align = "center"/>
  <img src="TH14.jpg"  width="350" align = "center"/>
  </div>
	  <br>
  </div>
</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>Downloads</h2>
  <div class="paper">
		<a href="https://github.com/zbwglory/MV-release"> Code </a> for extracting motion vector is released.
		<br>
    Models and Prototxts will be coming soon.
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Steps for implementing MV-CNN</h2>
  <div class="paper">
		1. Using motion vector <a href="https://github.com/zbwglory/MV-release"> code </a> to generate Motion Vector Images.
		<br>
		2. Generating the file list for training. The list should follow this <a href="https://github.com/yjxiong/caffe/blob/action_recog/examples/action_recognition/dataset_file_examples/train_flow_split1.txt"> format </a>
		<br>
		3. We use the VideoData layer as input and Clarifai Net (CNN-M-2048) with PReLU to train MV-CNN.
		<br>
		4. For data augmentation, random crop, random filp and multi-scale (scale_ratio: [1,.875,.75]) are used.
		<br>
		5. Following these steps, you should get 74.4% (MV-CNN train from scratch) for UCF-101 Split1.
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>References</h2>
  <div class="paper">
   B. Zhang, L. Wang, Z. Wang, Y. Qiao, and H. Wang, Real-time Action Recognition with Enhanced Motion Vector CNNs, in IEEE Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2016.
  </div>
</div>
</div>

<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=ccti&d=BhKvD3_Wrp-NPYpFxeYgl7m9_PYMTIQSxIS4Ao0lNBg"></script>


</div>
</div>

</body>
</html>
