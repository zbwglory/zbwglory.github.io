<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Cross-Modal and Hierarchical Modeling of Video and Text</title>
	<meta content="Bowen Zhang, zbwglory.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  background: #eee;
}

h1 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18pt;
  font-weight: 700;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  # font-size: 16px;
  font-weight:bold;
}

ul {
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

img.pipeline {
  float: center;
  width: 770px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'zbwglory.github.io');
  ga('send', 'pageview');

</script>
<body>

<div align = "center">
<h1>Cross-Modal and Hierarchical Modeling of Video and Text</h1> <br />
<h3>Bowen Zhang*, Hexiang Hu*, and Fei Sha</h3>
</div>

<div style="clear: both;">
<div class="paper">
  <img class="pipeline" src="illustration.png" title="" />
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Abstract</h2>
  <div class="paper">
		Visual data and text data are composed of information at multiple granularities. A video can describe a complex scene that is composed of multiple clips or shots, where each depicts a semantically coherent event or action. Similarly, a paragraph may contain sentences with diﬀerent topics, which collectively conveys a coherent message or story. In this paper, we investigate the modeling techniques for such hierarchical sequential data where there are correspondences across multiple modalities. Speciﬁcally, we introduce hierarchical sequence embedding (hse), a generic model for embedding sequential data of diﬀerent modalities into hierarchically semantic spaces, with either explicit or implicit correspondence information. We perform empirical studies on large-scale video and paragraph retrieval datasets and demonstrated superior performance by the proposed methods. Furthermore, we examine the eﬀectiveness of our learned embeddings when applied to downstream tasks. We show its utility in zero-shot action recognition and video captioning.
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Highlights</h2>
  <div class="paper">
		<ol>
  <li>Propose to hierarchically model cross-modal sequential data.</li>
	<li>Preserve correspondence of complex structures across modalities through discriminative losses and contrastive losses.</li>
	<li>State-of-the-art performance on video and paragraph retrieval.</li>
	<li>Systematical study on several tasks involving video and language.</li>
	</ol>
	<br>
  </div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>Methods</h2>
  <div class="paper">
		<div align=center> <img src="Method_1.png"  width="750" /> </div>
		<br>
		<br>
		<div align=center> <img src="Method_2.png"  width="750" /> </div>
	<br>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Tasks</h2>
  <div class="paper">
	<ol>
  <li>Video and Text Retrieval: <b> ActivityNet Dense Caption Dataset </b> and <b> Didmeo Dataset </b> </li>
	<li>Video Captioning: <b> ActivityNet Dense Caption Dataset </b> </li>
	<li>Zero-shot Action Recognition: <b> ActivityNet V1.3 </b> </li>
	</ol>
	</div>
</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>Downloads</h2>
  <div class="paper">
		1. <a href="0528.pdf"> Paper </a>;  2. <a href="0528-supp.pdf"> Suppl.</a> ;  3. <a href="Poster_ECCV2018_BZHHFS_version1.pdf"> Poster </a>;  4. Code (Coming soon)
		<br>
  </div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>References</h2>
  <div class="paper">
   B. Zhang*, H. Hu* and F. Sha, Cross-Modal and Hierarchical Modeling of Video and Text, in European Conference on Computer Vision <strong>(ECCV)</strong>, 2018.
  </div>
</div>
</div>

<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=ccti&d=BhKvD3_Wrp-NPYpFxeYgl7m9_PYMTIQSxIS4Ao0lNBg"></script>


</div>
</div>

</body>
</html>
