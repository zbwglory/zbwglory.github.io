<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Bowen Zhang</title>
	<meta content="Bowen Zhang, zbwglory.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight:bold;
}

ul {
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'zbwglory.github.io');
  ga('send', 'pageview');

</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>
<body>

<div style="clear: both;">
<div class="section">
<h2 id="preprint">Preprint</h2>

<div class="paper" id="cover_arxiv">
<strong>Co-training Transformer with Videos and Images Improves Action Recognition</strong><br />
<strong>Bowen Zhang</strong>, Jiahui Yu, Christopher Fifty, Wei Han, Andrew M. Dai, Ruoming Pang, and Fei Sha <br />
arXiv, 2021. <br />
[ <a href='https://arxiv.org/pdf/2112.07175.pdf'>Paper</a> ]  <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="hammer_arxiv">
<strong>A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus</strong><br />
<strong>Bowen Zhang*</strong>, Hexiang Hu*, Joonseok Lee, Ming Zhao, Sheide Chammas, Vihan Jain, Eugene Ie, and Fei Sha <br />
arXiv, 2020. <br />
[ <a href='https://arxiv.org/pdf/2011.09046.pdf'>Paper</a> ]  <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="odas_arxiv">
<strong>Online Action Detection in Streaming Videos with Time Buffers</strong><br />
<strong>Bowen Zhang</strong>, Hao Chen, Meng Wang, and Yuanjun Xiong<br />
arXiv, 2020. <br />
[ <a href='https://arxiv.org/pdf/2010.03016.pdf'>Paper</a> ]  <br />
</div>
<div class="spanner"></div>
</div>



<h2 id="confpapers">Conferences</h2>
<div class="paper" id="gscan_emnlp">
  <strong>Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?</strong><br />
  Linlu Qiu, Hexiang Hu, <strong>Bowen Zhang</strong>, Pete Shaw, and Fei Sha <br />
  Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2021. <alert>(Short paper, Oral Presentation) </alert> <br />
  [ <a href='https://arxiv.org/abs/2109.12243'>Paper</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="crg_emnlp">
  <strong>Visually Grounded Concept Composition</strong><br />
  <strong>Bowen Zhang</strong>, Hexiang Hu, Linlu Qiu, Pete Shaw, and Fei Sha <br />
  Findings of EMNLP, 2021. <br />
  [ <a href='https://arxiv.org/abs/2109.14115'>Paper</a> ] <br />
</div>
<div class="spanner"></div>
</div>
<div class="paper" id="emnlp20">
<strong>Learning to Represent Image and Text with Denotation Graph</strong><br />
<strong>Bowen Zhang</strong>, Hexiang Hu, Vihan Jain, Eugene Ie, and Fei Sha <br />
Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2020. <alert>(Long Paper, Oral Presentation) </alert> <br />
[ <a href='https://arxiv.org/abs/2010.02949'>Paper</a> ] [Code]  <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="emnlp">
<strong>A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images</strong><br />
Melissa Ailem, <strong>Bowen Zhang</strong>, Aurélien Bellet, Pascal Denis, and Fei Sha <br />
Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2018. <br />
[ <a href='https://aclanthology.org/D18-1177/'>Paper</a> ]  <br />
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="eccv_CHAE">
<strong>Cross-Modal and Hierarchical Modeling of Video and Text</strong><br />
<strong>Bowen Zhang*</strong>, Hexiang Hu*, and Fei Sha <br />
European Conference on Computer Vision (<strong>ECCV</strong>), 2018. <br />
[ <a href='https://arxiv.org/abs/1810.07212'>Paper</a>  ] [<a href='CMHSE/index.html'>Project page</a>]<br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="CVPR16">
<strong>Real-time Action Recognition with Enhanced Motion Vector CNNs</strong><br />
<strong>Bowen Zhang</strong>, Limin Wang, Zhe Wang, Yu Qiao, and Hanli Wang <br />
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2016. <br />
<alert>Real-time action recogntion with high performance.</alert> <br />
[ <a href='https://arxiv.org/abs/1604.07669'>Paper</a> ]  [ <a href='MV-CNN/index.html'>Project page</a> ] <br />
<div class="spanner"></div>
</div>

<div class="paper" id="VCIP2015">
<strong>Encoding Scale into Fisher Vector for Human Action Recognition</strong><br />
<strong>Bowen Zhang</strong> and Hanli Wang <br />
IEEE Conference on Visual Communications and Image Processing (<strong>VCIP</strong>), 2015. <br />
<alert>Oral Presentation.</alert> <br />
[ <a href='papers/VCIP2015_ZhangW.pdf'>Paper</a>  ] [ <a href='presentations/VCIP2015_Presentation_ZhangW.pdf'>Presentation</a> ] <br />
<div class="spanner"></div>
</div>

<h2 id="journals">Journals</h2>
<div class="paper" id="tip_DTMV">
<strong>Real-Time Action Recognition with Deeply-Transferred Motion Vector CNNs</strong><br />
<strong>Bowen Zhang</strong>, Limin Wang, Zhe Wang, Yu Qiao, and Hanli Wang <br />
IEEE Transaction on Image Processing (<strong>TIP</strong>). <br />
[ <a href='https://ieeexplore.ieee.org/abstract/document/8249882'>Paper</a> ]  <br />
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="tip_scene">
<strong>Weakly Supervised PatchNets: Describing and Aggregating Local Patches for Scene Recognition</strong><br />
Zhe Wang, Limin Wang, Yali Wang, <strong>Bowen Zhang</strong>, and Yu Qiao <br />
IEEE Transaction on Image Processing (<strong>TIP</strong>).<br />
[ <a href='http://arxiv.org/abs/1609.00153'>Paper</a>  ]  [ <a href='https://github.com/wangzheallen/vsad'>Code& Model</a>  ]<br />
</div>
<div class="spanner"></div>
</div>

<h2 id="workshops">Workshops</h2>
<div class="paper" id="iccv_ws_storytelling">
<strong>Visual Storytelling via Predicting Anchor Word Embeddings in the Stories</strong><br />
<strong>Bowen Zhang</strong>, Hexiang Hu, and Fei Sha <br />
ICCV 2019 Workshop on Closing the Loop Between Vision and Language, 2019. <br />
[ <a href='https://arxiv.org/abs/2001.04541'>Paper</a> ]  <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="arxiv_topic">
<strong>Topic Augmented Generator for Abstractive Summarization </strong><br />
Melissa Ailem, <strong>Bowen Zhang</strong>, and Fei Sha <br />
arxiv 1908.07026 (Short version is on BayLearn 2019). <br />
[ <a href='https://arxiv.org/abs/1908.07026'>Paper</a> ]  <br />
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="XiongW_Anet16">
<strong>CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2017</strong><br />
Yue Zhao, <strong>Bowen Zhang</strong>, Zhirong Wu, Shuo Yang, Lei Zhou, Sijie Yan, Limin Wang, Yuanjun Xiong, Dahua Lin, Yu Qiao, and Xiaoou Tang  <br />
ActivityNet Large Scale Activity Recognition Challenge, in conjuction with <strong>CVPR</strong>, 2017. <br />
[ <a href='https://arxiv.org/abs/1710.08011'>Paper</a>  ]  <br />
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="XiongW_Anet16">
<strong>CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016</strong><br />
Yuanjun Xiong, Limin Wang, Zhe Wang, <strong>Bowen Zhang</strong>, Hang Song, Wei Li, Dahua Lin, Yu Qiao, Luc Van Gool, and Xiaoou Tang  <br />
ActivityNet Large Scale Activity Recognition Challenge, in conjuction with <strong>CVPR</strong>, 2016. <br />
[ <a href='https://arxiv.org/abs/1608.00797'>Paper</a>  ] [ <a href='https://github.com/yjxiong/anet2016-cuhk'>Code</a>  ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="MediaEval15">
<strong>MIC-TJU in MediaEval 2015 Affective Impact of Movies Task</strong><br />
Yun. Yi, Hanli Wang, <strong>Bowen Zhang</strong>, and Jian Yu <br />
MediaEval 2015 Workshop. <br />
<alert>Top Performance.</alert> <br />
[ <a href='papers/VSD2015_YWZhangY.pdf'> Paper</a> ] <br />
<div class="spanner"></div>
</div>

<div class="paper" id="MediaEval14">
<strong>MIC-TJU at MediaEval Violent Scenes Detection (VSD) 2014</strong><br />
<strong>Bowen Zhang</strong>, Yun. Yi, Hanli Wang, and Jian Yu <br />
MediaEval 2014 Workshop. <br />
<alert>Rank 4th in Main Task and Generalization Task.</alert> <br />
[ <a href='papers/VSD2014_ZhangYWY.pdf'>Paper</a> ] [ <a href='presentations/VSD2014_Presentation_ZhangYWY.pdf'>Presentation</a> ] <br />
<div class="spanner"></div>
</div>

</div>
</div>


<hr>
</body>
</html>
